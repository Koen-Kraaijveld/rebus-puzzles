{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6559f3d-d34a-47ee-8c6b-20977b4b48c9",
   "metadata": {},
   "source": [
    "# Open-source Experiments\n",
    "\n",
    "In this notebook, we will show you how you can run the open-source models from our experiments on COLUMBUS. These models are: Fuyu-8b (non-instruction-tuned VQA), BLIP-2 Flan-T5-XXL (instruction-tuned VQA), and Mistral-7B (text-only instruction-tuned QA). All results computed in this notebook will be stored under the `model_results` folder (found in the same directory as this notebook). Additionally, all models will be downloaded under `model_downloads`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb75cb3-a523-482e-acf7-133a1180f1a5",
   "metadata": {},
   "source": [
    "## Setup\n",
    "To get started, run the setup as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570c21c2-97d4-4efa-8b37-bd52c2ede5ef",
   "metadata": {},
   "source": [
    "<!-- Run these commands to install necessary libraries -->\n",
    "%pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e7e2c8-e65e-4c16-8f63-2f9c7fe311de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add root folder to allow module imports\n",
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedf0938-eb4c-40a3-abdf-21dcf2bf534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python modules\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# Import COLUMBUS benckmark\n",
    "from puzzles.Benchmark import Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f60a91-a85c-47ac-87f1-13b72ea8f75d",
   "metadata": {},
   "source": [
    "The following code can be tweaked to alter the number of puzzles which the models will run on (first *n* puzzles), as well as the prompts that will be used. These prompts correspond to prompt 2 (see the paper) for the vision-language models, and prompt 3 for the text-only models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5fd730-5817-45b1-9191-edf352ca663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PUZZLES = 1008\n",
    "MODELS_DIR = \"./model_downloads\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"You are given a rebus puzzle. It consists of text or icons that is used to convey a word or phrase. It needs to be solved through creative thinking. Which word/phrase is conveyed in this image from the following options (either A, B, C, or D)?\\n(A) {} (B) {} (C) {} (D) {}\"\n",
    "MISTRAL_PROMPT_TEMPLATE = \"You are given a description of a graph that is used to convey a word or phrase. It needs to be solved through creative thinking. The nodes are elements that contain text or icons, which are then manipulated through the attributes of their node. The description is as follows:\\n{}\\nWhich word/phrase is conveyed in this description from the following options (either A, B, C, or D)?\\n(A) {} (B) {} (C) {} (D) {}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bc29d2-7055-4e38-a7d6-8b9fc8c631ec",
   "metadata": {},
   "source": [
    "## Non-instruction-tuned VQA Model (Fuyu-8b)\n",
    "\n",
    "The following code will download and run Fuyu-8b on COLUMBUS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6eae69-d1e0-42fb-a379-b2696a7249da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model and processor for Fuyu-8b\n",
    "model = FuyuForCausalLM.from_pretrained(\n",
    "    \"adept/fuyu-8b\",\n",
    "    cache_dir=MODELS_DIR,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "processor = FuyuProcessor.from_pretrained(\n",
    "    \"adept/fuyu-8b\",\n",
    "    cache_dir=MODELS_DIR\n",
    ")\n",
    "\n",
    "# Get puzzles from the benchmark\n",
    "benchmark = Benchmark(with_metadata=True)\n",
    "puzzles = benchmark.get_puzzles()\n",
    "\n",
    "# Loop over N_PUZZLES puzzles and prompt Fuyu-8b to solve the given puzzle\n",
    "for puzzle in tqdm(puzzles, desc=f\"Prompting Fuyu-8b\"):\n",
    "    # Get path to image and options\n",
    "    image = Image.open(puzzle[\"image\"]).convert(\"RGB\")\n",
    "    options = puzzle[\"options\"]\n",
    "\n",
    "    # Format prompt\n",
    "    prompt_format = list(options.values())\n",
    "    prompt = PROMPT_TEMPLATE.format(*prompt_format)\n",
    "    puzzle[\"prompt\"] = prompt\n",
    "\n",
    "    # Prompt Fuyu-8b\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device=DEVICE, dtype=torch.float16)\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=200)\n",
    "    generated_text = processor.batch_decode(generated_ids[:, -200:], skip_special_tokens=True)[0].strip()\n",
    "    puzzle[\"output\"] = generated_text\n",
    "\n",
    "# Save results under the 'model_results' folder\n",
    "with open(f\"./model_results/fuyu_prompt_2.json\", \"w\") as file:\n",
    "    json.dump(puzzles, file, indent=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c2efed-7e97-4276-bd0f-e726cac5aa28",
   "metadata": {},
   "source": [
    "## Instruction-tuned VQA Model (BLIP-2 Flan-T5-XXL)\n",
    "\n",
    "The following code will download and run BLIP-2 Flan-T5-XXL on COLUMBUS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c40c063-756c-4dc3-8346-dfa30847e6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model and processor for BLIP-2 Flan-T5-XXL\n",
    "processor = Blip2Processor.from_pretrained(\n",
    "    f\"Salesforce/blip2-flan-t5-xxl\",\n",
    "    cache_dir=models_dir\n",
    ")\n",
    "\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    f\"Salesforce/blip2-flan-t5-xxl\",\n",
    "    cache_dir=models_dir,\n",
    "    device_map={\"\": 0},\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Get puzzles from the benchmark\n",
    "benchmark = Benchmark(with_metadata=True)\n",
    "puzzles = benchmark.get_puzzles()\n",
    "\n",
    "# Loop over N_PUZZLES puzzles and prompt BLIP-2 Flan-T5-XXL to solve the given puzzle\n",
    "for puzzle in tqdm(puzzles, desc=f\"Prompting BLIP-2 Flan-T5-XXL\"):\n",
    "    # Get path to image and options\n",
    "    image = Image.open(puzzle[\"image\"]).convert(\"RGB\")\n",
    "    options = puzzle[\"options\"]\n",
    "    \n",
    "    # Format prompt\n",
    "    prompt_format = list(options.values())\n",
    "    prompt = PROMPT_TEMPLATE.format(*prompt_format)\n",
    "    puzzle[\"prompt\"] = prompt\n",
    "\n",
    "    # Prompt BLIP-2 FLan-T5-XXL\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device=DEVICE, dtype=torch.float16)\n",
    "    generated_ids = model.generate(**inputs, max_length=512)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "    puzzle[\"output\"] = generated_text\n",
    "\n",
    "# Save results under the 'model_results' folder\n",
    "with open(f\"./model_results/blip2-flan-t5-xxl_prompt_2.json\", \"w\") as file:\n",
    "    json.dump(puzzles, file, indent=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09b9baf-bbd5-4f05-bc48-f4d66cce39e2",
   "metadata": {},
   "source": [
    "## Instruction-tuned text-only QA Model (Mistral-7b)\n",
    "\n",
    "The following code will download and run Mistral-7b on COLUMBUS. This requires an API key for authentication, which by default is under the `MISTRAL_API_KEY` environmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066ec1fb-9e79-44ea-bf5c-abe710c626f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable for Mistral API key (change this is you do not have the environment variable set)\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "# Download model and processor for Mistral-7B\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    cache_dir=models_dir,\n",
    "    token=MISTRAL_API_KEY\n",
    ").to(DEVICE)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    cache_dir=models_dir,\n",
    "    token=MISTRAL_API_KEY\n",
    ")\n",
    "\n",
    "# Get puzzles from the benchmark\n",
    "benchmark = Benchmark(with_metadata=True)\n",
    "puzzles = benchmark.get_puzzles()\n",
    "\n",
    "# Loop over N_PUZZLES puzzles and prompt Mistral-7B to solve the given puzzle\n",
    "for puzzle in tqdm(puzzles, desc=f\"Prompting Mistral-7B\"):\n",
    "    # Get options\n",
    "    options = puzzle[\"options\"]\n",
    "\n",
    "    # Format prompt\n",
    "    prompt_format = [puzzle[\"metadata\"][\"nodes\"]] + list(options.values())\n",
    "    prompt = MISTRAL_PROMPT_TEMPLATE.format(*prompt_format)\n",
    "    puzzle[\"prompt\"] = prompt\n",
    "\n",
    "    # Mistral-7B\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(DEVICE)\n",
    "    generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)\n",
    "    generated_text = tokenizer.batch_decode(generated_ids)[0]\n",
    "    puzzle[\"output\"] = generated_text\n",
    "\n",
    "# Save results under the 'model_results' folder\n",
    "with open(f\"./model_results/blip2-flan-t5-xxl_prompt_2.json\", \"w\") as file:\n",
    "    json.dump(puzzles, file, indent=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
