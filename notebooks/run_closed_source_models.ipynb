{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0d29afb-b49a-49f0-a5dd-ee41cb882c7e",
   "metadata": {},
   "source": [
    "# Closed-source Experiments\n",
    "\n",
    "In this notebook, we will show you how you can run the closed-source models from our experiments on COLUMBUS. These models are: **GPT-4o**, **GPT-4o (mini)**, **Gemini 1.5 Pro**, and **Gemini 1.5 Flash**. All results computed in this notebook will be stored under the `model_results` folder (found in the same directory as this notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1ebb70-6a58-4200-9677-c2b7fdec21f4",
   "metadata": {},
   "source": [
    "## Setup\n",
    "To get started, run the setup as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01809d7-7942-414e-bd29-168eed90e4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add root folder to allow module imports\n",
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24376e3d-614f-45a0-9f61-6d981e5fa0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python modules\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Import tqdm for progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import COLUMBUS benckmark\n",
    "from puzzles.Benchmark import Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0038ef75-8085-46b0-93bc-05b190bbaa9a",
   "metadata": {},
   "source": [
    "The following code can be tweaked to alter the number of puzzles which the models will run on (first *n* puzzles), as well as the prompts that will be used. These prompts correspond to prompt 2 from the paper, which includes a description on the nature of the puzzle being solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814f8742-400e-415e-8a1b-e8d08a3b4514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of puzzles to run models on (defined as first n puzzles) \n",
    "# This variable must be between 1 and 1008 (1008 is the number of puzzles in COLUMBUS)\n",
    "N_PUZZLES = 1\n",
    "assert N_PUZZLES > 0 and N_PUZZLES <= 1008\n",
    "\n",
    "# Prompt used for regular prompting (i.e., without forward/backward chaining)\n",
    "PROMPT_TEMPLATE = \"You are given a rebus puzzle. It consists of text or icons that is used to convey a word or phrase. It needs to be solved through creative thinking. Which word/phrase is conveyed in this image from the following options (either A, B, C, or D)?\\n(A) {} (B) {} (C) {} (D) {}\"\n",
    "\n",
    "# Prompts used for forward chaining\n",
    "JSON_PROMPT = \"This image is a rebus puzzle which contains more than one item. An item can be text, icons or shapes. Describe all items in the image in a json file. Each item should have a sepearte json file. The json file should have the following format: {'id': 'temperal_id', 'name' : 'name_of_the_item', 'description': 'description_of_the_item', 'relation': 'location_relation_to_other_items','specialy': 'specialy_of_the_item_if_any'}. The 'specialy' field should be something you found special or different from normal items.\"\n",
    "REASON_PROMPT_TEMPLATE = \"\"\"The image is a rebus puzzle which contain a word or phrases. Here is a json file describing the item in the image.\n",
    "{}\n",
    "Which word/phrase is conveyed in this image from the following options (either A, B, C, or D)? \\n(A) {} (B) {} (C) {} (D) {}\n",
    "Start your reasoning by mapping each json file to the word in the answer choices. Only pick the answer choice that is consistent with all the json file.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38a0a88-a08d-4e4e-8f98-fb6b3f003ff4",
   "metadata": {},
   "source": [
    "## OpenAI Models (GPT-4o + GPT-4o-mini)\n",
    "\n",
    "The following section will run GPT-4o and GPT-4o-mini on COLUMBUS. This requires an API key provided by OpenAI, which you can change below. By default, this will select an environment variable under the name `OPENAI_API_KEY`, but you can also set this to a string defining the API key. You can also change the `GPT4_MODEL` variable to switch between GPT-4o and GPT-4o-mini (this must be equal to either `gpt-4o` or `gpt-4o-mini`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b740132-1c45-451b-a7f7-2bdcd80d66bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable for OpenAI API key (change this if you do not have the environment variable set)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Variable for which GPT-4o model to run (either \"gpt-4o\" or \"gpt-4o-mini\")\n",
    "GPT4_MODEL = \"gpt-4o\"\n",
    "assert GPT4_MODEL == \"gpt-4o\" or GPT4_MODEL == \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89c60ea-9639-4613-9c8e-93da4520e626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function to prompt GPT-4o API\n",
    "from notebook_util import prompt_gpt4\n",
    "\n",
    "# Get puzzles from the benchmark\n",
    "benchmark = Benchmark(with_metadata=True)\n",
    "puzzles = benchmark.get_puzzles()\n",
    "\n",
    "# Loop over N_PUZZLES puzzles and prompt GPT-4o(-mini) to solve the given puzzle\n",
    "for puzzle in tqdm(puzzles[:N_PUZZLES], desc=f\"Prompting {GPT4_MODEL}\"):\n",
    "    # Get path to image and options\n",
    "    image = puzzle[\"image\"]\n",
    "    options = puzzle[\"options\"]\n",
    "\n",
    "    # Format the prompt\n",
    "    prompt_format = list(options.values())\n",
    "    prompt = PROMPT_TEMPLATE.format(*prompt_format)\n",
    "    puzzle[\"prompt\"] = prompt\n",
    "\n",
    "    # Prompt GPT-4o(-mini)\n",
    "    response = prompt_gpt4(prompt, image, GPT4_MODEL, OPENAI_API_KEY)\n",
    "    puzzle[\"output\"] = response\n",
    "\n",
    "# Save results under the 'model_results' folder\n",
    "with open(f\"./model_results/{GPT4_MODEL.lower()}_prompt_2.json\", \"w\") as file:\n",
    "    json.dump(puzzles, file, indent=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caab6581-8a57-4e1f-990b-a6c7f132ace6",
   "metadata": {},
   "source": [
    "### Forward chaining\n",
    "\n",
    "The following code will run the forward chaining variant of GPT-4o(-mini) on COLUMBUS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6ba3ce-2cdc-45ff-9da9-bc9e3b3c1b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function to prompt GPT-4o API\n",
    "from notebook_util import prompt_gpt4\n",
    "\n",
    "# Get puzzles from the benchmark\n",
    "benchmark = Benchmark(with_metadata=True)\n",
    "puzzles = benchmark.get_puzzles()\n",
    "\n",
    "# Loop over N_PUZZLES puzzles and prompt GPT-4o(-mini) to solve the given puzzle\n",
    "for puzzle in tqdm(puzzles[:N_PUZZLES], desc=f\"Prompting {GPT4_MODEL} (forward chaining)\"):\n",
    "    # Get path to image and options\n",
    "    image = puzzle[\"image\"]\n",
    "    options = puzzle[\"options\"]\n",
    "\n",
    "    # Prompt GPT-4o(-mini) for a JSON file with information on the puzzle\n",
    "    json_response = prompt_gpt4(JSON_PROMPT, image, GPT4_MODEL, OPENAI_API_KEY)    \n",
    "\n",
    "    # Prompt GPT-4o(-mini) to solve the puzzle given the JSON file\n",
    "    prompt_format = [json_response] + list(options.values())\n",
    "    prompt = REASON_PROMPT_TEMPLATE.format(*prompt_format)\n",
    "    puzzle[\"prompt\"] = prompt\n",
    "    response = prompt_gpt4(prompt, image, GPT4_MODEL, OPENAI_API_KEY)\n",
    "    puzzle[\"output\"] = response\n",
    "\n",
    "# Save results under the 'model_results' folder\n",
    "with open(f\"./model_results/{GPT4_MODEL.lower()}_fc_prompt_2.json\", \"w\") as file:\n",
    "    json.dump(puzzles, file, indent=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2673e16-5833-40fc-9562-f7857b827a32",
   "metadata": {},
   "source": [
    "### Backward Chaining\n",
    "\n",
    "The following code will run the backward chaining variant (through belief graphs) of GPT-4o(-mini) on COLUMBUS. \n",
    "\n",
    "For this approach, there is also an opportunity to visualize the belief graph after it has been fixed (set `visualize_graph` to `True` to do so). This requires pygraphviz, which can be be installed through conda: `conda install --channel conda-forge pygraphviz`. The graph can be interpreted as follows:\n",
    "- Square nodes (ðŸŸ© or ðŸŸ¥) are statement nodes, which has the following properties:\n",
    "    - They are a statement expressing a belief (i.e., does the model believe X to be true?).\n",
    "    - They contain a truth assignment (<span style=\"color:green\">green</span> = true, <span style=\"color:red\">red</span> = false).\n",
    "    - They contain a confidence that the assigned truth value is accurate (between 0 and 1).\n",
    "- Circular nodes (âš«) are rule nodes, which have the following properties:\n",
    "    - They connect statement nodes to each other. Statement nodes from incoming edges represent premises, and the statement node from the outgoing edge represent the hypothesis.\n",
    "    - They contain a confidence that the premises imply the hypothesis (between 0 and 1).\n",
    "  \n",
    "\n",
    "**NOTE**: this approach will incur high API costs, as solving a single puzzle will usually require around 20+ prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2665cb-fc9e-4a27-90b4-dd5baa15e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import code necessary to run the backward chaining approach through belief graphs.\n",
    "from graphs.BeliefGraphGenerator import BeliefGraphGenerator\n",
    "from graphs.BeliefGraphReasoner import BeliefGraphReasoner\n",
    "\n",
    "# Set the seed\n",
    "seed = 43\n",
    "random.seed(seed)\n",
    "\n",
    "# Load benchmark and sample min(N_PUZZLES, 50) random puzzles\n",
    "benchmark = Benchmark()\n",
    "puzzles = random.sample(benchmark.get_puzzles(), min(N_PUZZLES, 50))\n",
    "\n",
    "# Define hyperparameters\n",
    "max_depth = 1\n",
    "hyperparameters = {\n",
    "    \"k\": 9,\n",
    "    \"k_entailer\": 36,\n",
    "    \"k_xor\": 30,\n",
    "    \"k_mc\": 9,\n",
    "    \"t_entailer\": 1.02,\n",
    "    \"t_xor\": 1.1,\n",
    "    \"t_mc\": 0.98,\n",
    "    \"m_xor\": 0.3,\n",
    "    \"c_xor\": 1.,\n",
    "    \"c_mc\": 1.\n",
    "}\n",
    "\n",
    "visualize_graph = True\n",
    "\n",
    "# Loop over puzzles, generate a belief graph for it, and optimize the graph to solve the given puzzle\n",
    "for puzzle in tqdm(puzzles, desc=f\"Prompting {GPT4_MODEL} (backward chaining)\"):\n",
    "    # Get path to image and options\n",
    "    image = puzzle[\"image\"]\n",
    "    options = list(puzzle[\"options\"].values())\n",
    "\n",
    "    # Generate a belief graph\n",
    "    generator = BeliefGraphGenerator(image, 0, options, hyperparameters, max_depth=max_depth, model=GPT4_MODEL)\n",
    "    graph = generator.generate_graph()\n",
    "\n",
    "    # Optimize belief graph by fixing logical conflicts\n",
    "    reasoner = BeliefGraphReasoner(hyperparameters)\n",
    "    graph, _ = reasoner.fix_graph(graph)\n",
    "\n",
    "    if visualize_graph:\n",
    "        print(graph)\n",
    "        graph.visualize(show=True)\n",
    "\n",
    "    # Compute answer\n",
    "    answer_csp = graph.get_answer()\n",
    "    puzzle[\"output\"] = answer_csp\n",
    "\n",
    "# Save results under the 'model_results' folder\n",
    "with open(f\"./model_results/{GPT4_MODEL.lower()}_bc_prompt_2.json\", \"w\") as file:\n",
    "    json.dump(puzzles, file, indent=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31288ea2-e54f-4b5a-8d9a-2c60077995c4",
   "metadata": {},
   "source": [
    "## Google Models (Gemini 1.5 Pro + Flash)\n",
    "\n",
    "Similarly, the following section will run Gemini 1.5 Pro and Gemini 1.5 Flash on COLUMBUS. This requires an API key provided by Google, which you can change below. By default, this will select an environment variable under the name `GOOGLE_API_KEY`, but you can also set this to a string defining the API key. You can also change the `GEMINI_MODEL` variable to switch between Gemini 1.5 Pro and Flash (this must be equal to either `gemini-1.5-pro` or `gemini-1.5-flash`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e269d81-c19e-427a-af9a-70bcf1ac8997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable for Google API key (change this is you do not have the environment variable set)\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Variable for which Gemini model to run (either \"gemini-1.5-pro\" or \"gemini-1.5-flash\")\n",
    "GEMINI_MODEL = \"gemini-1.5-pro\"\n",
    "assert GEMINI_MODEL == \"gemini-1.5-pro\" or GEMINI_MODEL == \"gemini-1.5-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893f0f3a-ef6f-41e5-81c5-ab3b3a58343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function to prompt the Gemini API\n",
    "from notebook_util import prompt_gemini\n",
    "\n",
    "# Get puzzles from the benchmark\n",
    "benchmark = Benchmark(with_metadata=True)\n",
    "puzzles = benchmark.get_puzzles()\n",
    "\n",
    "# Loop over N_PUZZLES puzzles and prompt Gemini-1.5 to solve the given puzzle\n",
    "for puzzle in tqdm(puzzles[:N_PUZZLES], desc=f\"Prompting {GEMINI_MODEL}\"):\n",
    "    # Get path to image and options\n",
    "    image = puzzle[\"image\"]\n",
    "    options = puzzle[\"options\"]\n",
    "\n",
    "    # Format prompt\n",
    "    prompt_format = list(options.values())\n",
    "    prompt = PROMPT_TEMPLATE.format(*prompt_format)\n",
    "    puzzle[\"prompt\"] = prompt\n",
    "\n",
    "    # Prompt Gemini 1.5 (Pro/Flash)\n",
    "    response = prompt_gemini(prompt, image, GEMINI_MODEL, GOOGLE_API_KEY, verbose=True)\n",
    "    puzzle[\"output\"] = response\n",
    "\n",
    "# Save results under the 'model_results' folder\n",
    "with open(f\"./model_results/{GEMINI_MODEL.lower()}_prompt_2.json\", \"w\") as file:\n",
    "    json.dump(puzzles, file, indent=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc782167-1716-4747-b1c1-2154de9630b8",
   "metadata": {},
   "source": [
    "### Forward chaining\n",
    "\n",
    "The following code will run the forward chaining variant of Gemini 1.5 Pro/Flash on COLUMBUS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecefe685-b015-4661-b348-45101c9c91f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function to prompt the Gemini API\n",
    "from notebook_util import prompt_gemini\n",
    "\n",
    "# Get puzzles from the benchmark\n",
    "benchmark = Benchmark(with_metadata=True)\n",
    "puzzles = benchmark.get_puzzles()\n",
    "\n",
    "# Loop over N_PUZZLES puzzles and prompt Gemini-1.5 to solve the given puzzle\n",
    "for puzzle in tqdm(puzzles[:N_PUZZLES], desc=f\"Prompting {GEMINI_MODEL}\"):\n",
    "    # Get path to image and options\n",
    "    image = puzzle[\"image\"]\n",
    "    options = puzzle[\"options\"]\n",
    "\n",
    "    # Prompt Gemini 1.5 (Pro/Flash) for a JSON file with information on the puzzle\n",
    "    json_response = prompt_gemini(JSON_PROMPT, image, GEMINI_MODEL, GOOGLE_API_KEY)    \n",
    "\n",
    "    # Prompt Gemini 1.5 (Pro/Flash) to solve the puzzle given the JSON file\n",
    "    prompt_format = [json_response] + list(options.values())\n",
    "    prompt = REASON_PROMPT_TEMPLATE.format(*prompt_format)\n",
    "    puzzle[\"prompt\"] = prompt\n",
    "    response = prompt_gemini(prompt, image, GEMINI_MODEL, GOOGLE_API_KEY)\n",
    "    puzzle[\"output\"] = response\n",
    "\n",
    "# Save results under the 'model_results' folder\n",
    "with open(f\"./model_results/{GEMINI_MODEL.lower()}_fc_prompt_2.json\", \"w\") as file:\n",
    "    json.dump(puzzles, file, indent=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "columbus-test-v1",
   "language": "python",
   "name": "columbus-test-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
